{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Envpool\n",
        "\n",
        "Envpool is a recent work that offers accelerated vectorized environments for Atari by leveraging C++ and thread pools.\n"
      ],
      "metadata": {
        "id": "a2npAuKuNdVM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade gym[atari]\n",
        "!pip install stable_baselines3\n",
        "!pip install atari-py==0.2.5\n",
        "!pip install envpool"
      ],
      "metadata": {
        "id": "ziGQfAfJbIRS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import random\n",
        "import gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.distributions.categorical import Categorical\n",
        "from sklearn.model_selection import ShuffleSplit\n",
        "\n",
        "import envpool\n",
        "\n",
        "from stable_baselines3.common.atari_wrappers import (\n",
        "    ClipRewardEnv,\n",
        "    EpisodicLifeEnv,\n",
        "    FireResetEnv,\n",
        "    MaxAndSkipEnv,\n",
        "    NoopResetEnv,\n",
        ")"
      ],
      "metadata": {
        "id": "41HxvtcJgmvb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
        "    nn.init.orthogonal_(layer.weight, std)\n",
        "    nn.init.constant_(layer.bias, bias_const)\n",
        "    return layer"
      ],
      "metadata": {
        "id": "WHt_EgEWdxax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PPOAgent(nn.Module):\n",
        "    def __init__(self, action_space):\n",
        "        super(PPOAgent, self).__init__()\n",
        "\n",
        "        self.network = nn.Sequential(\n",
        "            layer_init(nn.Conv2d(4, 32, 8, stride=4)),\n",
        "            nn.ReLU(),\n",
        "            layer_init(nn.Conv2d(32, 64, 4, stride=2)),\n",
        "            nn.ReLU(),\n",
        "            layer_init(nn.Conv2d(64, 64, 3, stride=1)),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "            layer_init(nn.Linear(64*7*7, 512)),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        self.actor = layer_init(nn.Linear(512, action_space), std=0.01)\n",
        "        self.critic = layer_init(nn.Linear(512, 1), std=1.0)\n",
        "\n",
        "    def get_value(self, x):\n",
        "        hidden = self.network(x / 255.0)\n",
        "        return self.critic(hidden)\n",
        "\n",
        "    def get_action_and_value(self, x, action=None):\n",
        "        hidden = self.network(x / 255.0)\n",
        "        logits = self.actor(hidden)\n",
        "        probs = Categorical(logits=logits) # soft-max distribution\n",
        "\n",
        "        if action is None:\n",
        "            action = probs.sample()\n",
        "\n",
        "        log_prob = probs.log_prob(action)\n",
        "        entropy = probs.entropy()\n",
        "        value = self.critic(hidden)\n",
        "\n",
        "        return action, log_prob, entropy, value"
      ],
      "metadata": {
        "id": "XyCdbsxNfSeK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PPO:\n",
        "    def __init__(self, state_shape, action_shape, action_n, config):\n",
        "        self.state_shape = state_shape\n",
        "        self.action_shape = action_shape\n",
        "        self.config = config\n",
        "        self.agent = PPOAgent(action_n).to(config.device)\n",
        "        self.optimizer = optim.Adam(self.agent.parameters(), lr=config.learning_rate, eps=1e-5)\n",
        "\n",
        "\n",
        "    def select_action(self, state):\n",
        "        with torch.no_grad():\n",
        "            state = torch.Tensor(state).to(self.config.device)\n",
        "            action, logprob, _, value = self.agent.get_action_and_value(state)\n",
        "        return action, logprob, value.flatten()\n",
        "\n",
        "\n",
        "    def get_value(self, state):\n",
        "        with torch.no_grad():\n",
        "            state = torch.Tensor(state).to(self.config.device)\n",
        "            value = self.agent.get_value(state)\n",
        "        return value.reshape(1, -1)\n",
        "\n",
        "\n",
        "    def compute_loss(self, trajectory):\n",
        "        states, actions, returns, dones, values, logprobs, advantages = trajectory\n",
        "\n",
        "        # Advantages normalization\n",
        "        if self.config.norm_adv:\n",
        "            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
        "\n",
        "        # Predictions\n",
        "        _, logprobs_pred, entropy_pred, values_pred = self.agent.get_action_and_value(states, actions)\n",
        "\n",
        "        log_ratio = logprobs_pred - logprobs\n",
        "        ratio = log_ratio.exp()\n",
        "\n",
        "        if (log_ratio.abs() < 1e-6).all().item():\n",
        "            print(\"New and old policies are the same: ratio = 1.0\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            approx_kl = ((ratio - 1) - log_ratio).mean()\n",
        "            clip_frac = [((ratio - 1.0).abs() > self.config.clip_coef).float().mean().item()]\n",
        "\n",
        "        if approx_kl > self.config.target_kl:\n",
        "             print(f\"The policy is changing too quickly: approx_kl = {approx_kl}, which is more than {self.config.target_kl}\")\n",
        "\n",
        "        # Policy loss\n",
        "        pg_loss_unclipped = -advantages * ratio\n",
        "        pg_loss_clipped = -advantages * torch.clamp(ratio, 1 - self.config.clip_coef, 1 + self.config.clip_coef)\n",
        "        pg_loss = torch.max(pg_loss_unclipped, pg_loss_clipped).mean()\n",
        "\n",
        "        # Value loss\n",
        "        values_pred = values_pred.view(-1)\n",
        "\n",
        "        v_loss_sqrt = (values_pred - returns) ** 2 # Unclipped\n",
        "        if self.config.clip_vloss:\n",
        "            # Ensure that the value function updates do not deviate too far from the original values\n",
        "            v_clipped = torch.clamp(values_pred, values - self.config.clip_coef, values + self.config.clip_coef)\n",
        "            v_loss_clipped = (v_clipped - returns) ** 2\n",
        "            v_loss_sqrt = torch.max(v_loss_sqrt, v_loss_clipped)\n",
        "\n",
        "        v_loss = 0.5 * v_loss_sqrt.mean()\n",
        "\n",
        "        # Entropy Loss\n",
        "        entropy_loss = entropy_pred.mean()\n",
        "\n",
        "        # Overall Loss\n",
        "        loss = pg_loss - self.config.ent_coef * entropy_loss + self.config.vf_coef * v_loss\n",
        "\n",
        "        metrics = {\n",
        "            \"value_loss\": v_loss.item(),\n",
        "            \"policy_loss\": pg_loss.item(),\n",
        "            \"entropy_loss\": entropy_loss.item(),\n",
        "            \"approx_kl\": approx_kl,\n",
        "            \"clip_frac\": clip_frac,\n",
        "        }\n",
        "\n",
        "        return loss, metrics\n",
        "\n",
        "\n",
        "    def update(self, trajectory, next_value, update_idx):\n",
        "        states, actions, rewards, dones, values, logprobs = zip(*trajectory)\n",
        "\n",
        "        states = torch.tensor(states, dtype=torch.float32).to(self.config.device)\n",
        "        actions = torch.stack(actions).long().to(self.config.device)\n",
        "        rewards = torch.tensor(rewards, dtype=torch.float32).to(self.config.device)\n",
        "        dones = torch.tensor(dones, dtype=torch.float32).to(self.config.device)\n",
        "        values = torch.stack(values).to(self.config.device)\n",
        "        logprobs = torch.stack(logprobs).to(self.config.device)\n",
        "\n",
        "        # Update learning rate\n",
        "        if self.config.anneal_lr:\n",
        "            lr_frac = 1.0 - update_idx / self.config.num_updates\n",
        "            self.optimizer.param_groups[0][\"lr\"] = lr_frac * self.config.learning_rate\n",
        "\n",
        "        # Generalized Advantage Estimation\n",
        "        with torch.no_grad():\n",
        "            if self.config.gae:\n",
        "                returns, advantages = self.compute_gae_advantages(rewards, values, next_value, dones)\n",
        "            else:\n",
        "                returns, advantages = self.compute_n_step_return(rewards, values, next_value, dones)\n",
        "\n",
        "        # Flatten the batch: num_steps * num_envs\n",
        "        states = states.reshape((-1,) + self.state_shape)\n",
        "        actions = actions.reshape((-1,) + self.action_shape)\n",
        "        returns = returns.reshape(-1)\n",
        "        dones = dones.reshape(-1)\n",
        "        values = values.reshape(-1)\n",
        "        logprobs = logprobs.reshape(-1)\n",
        "        advantages = advantages.reshape(-1)\n",
        "\n",
        "        clip_fracs = []\n",
        "\n",
        "        for epoch in range(self.config.num_epochs):\n",
        "            minibatch_indices = self.get_minibatch_indices()\n",
        "            for inds in minibatch_indices:\n",
        "                trajectory = (states[inds], actions[inds], returns[inds], dones[inds], values[inds], logprobs[inds], advantages[inds])\n",
        "\n",
        "                self.optimizer.zero_grad()\n",
        "                loss, loss_metrics = self.compute_loss(trajectory)\n",
        "                clip_fracs.append(loss_metrics['clip_frac'])\n",
        "                loss.backward()\n",
        "\n",
        "                # Global Gradient Clipping\n",
        "                nn.utils.clip_grad_norm_(self.agent.parameters(), self.config.max_grad_norm)\n",
        "                self.optimizer.step()\n",
        "\n",
        "        y_pred, y_true = values.cpu().numpy(), returns.cpu().numpy()\n",
        "        var_y = np.var(y_true)\n",
        "        explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y\n",
        "\n",
        "        metrics = {\n",
        "            \"loss\": loss.item(),\n",
        "            \"learning_rate\": self.optimizer.param_groups[0][\"lr\"],\n",
        "            \"value_loss\": loss_metrics['value_loss'],\n",
        "            \"policy_loss\": loss_metrics['policy_loss'],\n",
        "            \"entropy\": loss_metrics['entropy_loss'],\n",
        "            \"approx_kl\": loss_metrics['approx_kl'],\n",
        "            \"clip_fracs\": np.mean(clip_fracs),\n",
        "            \"explained_variance\": explained_var\n",
        "        }\n",
        "\n",
        "        return metrics\n",
        "\n",
        "\n",
        "    ''' Generates mini-batches by shuffling and breaking envs in batches '''\n",
        "    def get_minibatch_indices(self):\n",
        "        batch_inds = np.arange(self.config.batch_size)\n",
        "\n",
        "        ss = ShuffleSplit(n_splits=self.config.num_minibatches, train_size=self.config.minibatch_size)\n",
        "        minibatch_inds = [i[0] for i in ss.split(batch_inds)]\n",
        "\n",
        "        return minibatch_inds\n",
        "\n",
        "\n",
        "    def compute_gae_advantages(self, rewards, values, next_return, dones):\n",
        "        gae_advantages = torch.zeros_like(rewards)\n",
        "\n",
        "        gae = 0\n",
        "        for t in reversed(range(self.config.num_steps)):\n",
        "            delta = rewards[t] + self.config.gamma * next_return * (1 - dones[t]) - values[t]\n",
        "            gae = delta + self.config.gamma * self.config.gae_lambda * gae * (1 - dones[t])\n",
        "            gae_advantages[t] = gae\n",
        "            next_return = values[t]\n",
        "\n",
        "        returns = gae_advantages + values\n",
        "        return returns, gae_advantages\n",
        "\n",
        "\n",
        "    def compute_n_step_return(self, rewards, values, next_return, dones):\n",
        "      returns = torch.zeros_like(rewards)\n",
        "\n",
        "      for t in reversed(range(self.config.num_steps)):\n",
        "          returns[t] = rewards[t] + self.config.gamma * next_return * (1 - dones[t])\n",
        "          next_return = returns[t]\n",
        "\n",
        "      advantages = returns - values\n",
        "      return returns, advantages"
      ],
      "metadata": {
        "id": "mT0AtY9bt5Xw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = argparse.Namespace()\n",
        "\n",
        "config.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "config.seed = 1\n",
        "\n",
        "config.env_id = 'Pong-v5'\n",
        "config.num_envs = 16\n",
        "config.num_steps = 128     # The number of steps to run in each env to policy rollout\n",
        "\n",
        "config.batch_size = config.num_envs * config.num_steps\n",
        "config.num_minibatches = 8 # the number of mini-batches\n",
        "config.minibatch_size = config.batch_size // config.num_minibatches\n",
        "\n",
        "config.total_timesteps = 10000000\n",
        "config.num_updates = config.total_timesteps // config.batch_size\n",
        "config.num_epochs = 3\n",
        "\n",
        "config.learning_rate = 3e-4\n",
        "config.gamma = 0.99\n",
        "config.anneal_lr = True    # Learning rate annealing for policy and value networks\n",
        "config.gae = True         # Generalized Advantage Estimation\n",
        "config.gae_lambda = 0.95\n",
        "\n",
        "config.norm_adv = True     # Advantages normalization\n",
        "config.clip_coef = 0.2     # The surrogate clipping coefficient (policy and value function)\n",
        "config.clip_vloss = True   # Use clip_coef to clip value function\n",
        "\n",
        "config.vf_coef = 0.5       # The value coefficient to calculate loss\n",
        "config.ent_coef = 0.01     # Encourages the policy to explore a diverse set of actions\n",
        "config.max_grad_norm = 0.5 # The maximum normalization for the gradient clipping\n",
        "config.target_kl = 0.015   # The target KL divergence threshold\n",
        "\n",
        "config.track = True\n",
        "config.record_video = True"
      ],
      "metadata": {
        "id": "DLKFq8qNvz0h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Connect to Weights and Biases"
      ],
      "metadata": {
        "id": "zMWeN-zCwPAn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if config.track:\n",
        "    !pip install wandb"
      ],
      "metadata": {
        "id": "FcGQTrTCdcFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if config.track:\n",
        "    import wandb\n",
        "    wandb.login()"
      ],
      "metadata": {
        "id": "FqEOt34wwJ1o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if config.track:\n",
        "    wandb.init(\n",
        "        project='ppo-implementation-details',\n",
        "        config=config,\n",
        "        name=f\"{config.env_id}_envpool\",\n",
        "        monitor_gym=True,\n",
        "        save_code=True,\n",
        "    )"
      ],
      "metadata": {
        "id": "z2jmW0zXkI7g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate envpool environment"
      ],
      "metadata": {
        "id": "2BjMMr86CJPX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_envpool(env_id, num_envs):\n",
        "    envs = envpool.make(config.env_id, env_type=\"gym\", num_envs=num_envs)\n",
        "    envs.num_envs = num_envs\n",
        "    envs.single_action_space = envs.action_space\n",
        "    envs.single_observation_space = envs.observation_space\n",
        "\n",
        "    return envs"
      ],
      "metadata": {
        "id": "Mc-EWkNb3Mya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Seeding\n",
        "random.seed(config.seed)\n",
        "np.random.seed(config.seed)\n",
        "torch.manual_seed(config.seed)\n",
        "\n",
        "envs = make_envpool(config.env_id, config.num_envs)\n",
        "\n",
        "print(\"Observation space:\", envs.single_observation_space)\n",
        "print(\"Action space:\", envs.single_action_space)\n",
        "\n",
        "assert isinstance(envs.single_action_space, gym.spaces.Discrete), \"only discrete action space is supported\"\n",
        "\n",
        "global_step = 0\n",
        "ppo = PPO(envs.single_observation_space.shape, envs.single_action_space.shape, envs.single_action_space.n, config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_IW5a-UjChA",
        "outputId": "87e5d1ca-d60a-482f-e3cd-7dd61e1d65b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Observation space: Box(0, 255, (4, 84, 84), uint8)\n",
            "Action space: Discrete(6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train"
      ],
      "metadata": {
        "id": "S_f86t1XCSJ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def PPOTrain(envs, config):\n",
        "  global global_step\n",
        "\n",
        "  state = envs.reset()[0]\n",
        "\n",
        "  for update in range(config.num_updates):\n",
        "      episodic_return = 0\n",
        "      trajectory = []\n",
        "\n",
        "      # Do n-steps\n",
        "      for step in range(config.num_steps):\n",
        "          global_step += config.num_envs\n",
        "\n",
        "          action, logprob, value = ppo.select_action(state)\n",
        "          next_state, reward, done, _, _ = envs.step(action.cpu().numpy())\n",
        "          trajectory.append((state, action, reward, done, value, logprob))\n",
        "\n",
        "          episodic_return += reward.mean()\n",
        "          state = next_state\n",
        "\n",
        "      print(f\"global_step={global_step}, episodic_return={episodic_return}\")\n",
        "      if config.track:\n",
        "          wandb.log({'episodic_return': episodic_return})\n",
        "\n",
        "      next_value = ppo.get_value(next_state)\n",
        "      metrics = ppo.update(trajectory, next_value, update)\n",
        "\n",
        "      if config.track:\n",
        "          wandb.log(metrics)\n",
        "          print(metrics)\n",
        "\n",
        "      # Early stop using approx_kl\n",
        "      if config.target_kl is not None:\n",
        "          if metrics['approx_kl'] > config.target_kl:\n",
        "              break"
      ],
      "metadata": {
        "id": "Nobj6RHd0hWf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PPOTrain(envs, config)"
      ],
      "metadata": {
        "id": "CL_nAYeRoHNp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "envs.close()\n",
        "if config.track: wandb.finish()"
      ],
      "metadata": {
        "id": "NSDXZEailmi2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}