{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### 13 core implementation details\n",
        "\n",
        "\n",
        "1.   Vectorized architecture:\n",
        "\n",
        "  PPO leverages an efficient paradigm known as the vectorized architecture that features a single learner that collects samples and learns from multiple environments.\n",
        "\n",
        "2.   Layer Initialization:\n",
        "\n",
        "  Orthogonal Initialization of Weights and Constant Initialization of biases\n",
        "\n",
        "3. The Adam Optimizer's Epsilon Parameter:\n",
        "\n",
        "  PPO sets the epsilon parameter to 1e-5\n",
        "\n",
        "4. Adam Learning Rate Annealing:\n",
        "\n",
        "  By default, the hyper-parameters for training agents playing Atari games set the learning rate to linearly decay from 2.5e-4 to 0. In MuJoCo, the learning rate linearly decays from 3e-4 to 0.\n",
        "\n",
        "5. Generalized Advantage Estimation:\n",
        "\n",
        "  GAE is used to estimate the advantages of taking different actions in a given state. The advantage function measures how much better or worse an action is compared to the average action in that state, and it is a crucial component for updating the policy in PPO.\n",
        "  Compute the advantage function using the n-step returns and the predicted rewards.\n",
        "\n",
        "  If a sub-environment is not terminated nor truncated, PPO estimates the value of the next state in this sub-environment as the value target.\n",
        "  PPO implements the return target as returns = advantages + values, which corresponds to TD(λ) for value estimation.\n",
        "\n",
        "6. Mini-batch Updates:\n",
        "\n",
        "  The PPO implementation shuffles the indices of the training data of size N*M and breaks it into mini-batches to compute the gradient and update the policy.\n",
        "\n",
        "7. Normalization of Advantages:\n",
        "\n",
        "  PPO normalizes the advantages by subtracting their mean and dividing them by their standard deviation. This normalization happens at the mini-batch level instead of the whole batch level.\n",
        "\n",
        "8. Clipped surrogate objective\n",
        "\n",
        "9. Value Function Loss Clipping:\n",
        "\n",
        "  Given the Vtarg = returns = advantages + values, PPO fits the the value network by minimizing the following loss:\n",
        "\n",
        "  ![image.png](data:image/png;base64,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)\n",
        "\n",
        "10. Overall Loss and Entropy bonus:\n",
        "\n",
        "  The overall loss is calculated as\n",
        "\n",
        "    loss = policy_loss - entropy * entropy_coefficient + value_loss * value_coefficient\n",
        "  \n",
        "  which maximizes an entropy bonus term. The policy parameters and value parameters share the same optimizer.\n",
        "\n",
        "11. Global Gradient Clipping:\n",
        "\n",
        "  For each update iteration in an epoch, PPO rescales the gradients of the policy and value network so that the global l2 norm does not exceed 0.5.\n",
        "\n",
        "12. Debug variables:\n",
        "\n",
        "  The PPO implementation comes with several debug variables, which are:\n",
        "\n",
        "    * policy_loss: the mean policy loss across all data points.\n",
        "    * value_loss: the mean value loss across all data points.\n",
        "    * entropy_loss: the mean entropy value across all data points.\n",
        "    * clipfrac: the fraction of the training data that triggered the clipped objective.\n",
        "    * approxkl: the approximate Kullback–Leibler divergence, measured by (-logratio).mean().\n",
        "\n",
        "13. Shared and separate MLP networks for policy and value functions:\n",
        "\n",
        "  By default, PPO uses a simple MLP network consisting of two layers of 64 neurons and Hyperbolic Tangent as the activation function. Then PPO builds a policy head and value head that share the outputs of the MLP network.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "a2npAuKuNdVM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import random\n",
        "import gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.distributions.categorical import Categorical"
      ],
      "metadata": {
        "id": "41HxvtcJgmvb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
        "    nn.init.orthogonal_(layer.weight, std)\n",
        "    nn.init.constant_(layer.bias, bias_const)\n",
        "    return layer"
      ],
      "metadata": {
        "id": "WHt_EgEWdxax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PPOAgent(nn.Module):\n",
        "    def __init__(self, envs):\n",
        "        super(PPOAgent, self).__init__()\n",
        "        self.actor = nn.Sequential(\n",
        "            layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64)),\n",
        "            nn.Tanh(),\n",
        "            layer_init(nn.Linear(64, 64)),\n",
        "            nn.Tanh(),\n",
        "            layer_init(nn.Linear(64, envs.single_action_space.n), std=0.01)\n",
        "        )\n",
        "\n",
        "        self.critic = nn.Sequential(\n",
        "            layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64)),\n",
        "            nn.Tanh(),\n",
        "            layer_init(nn.Linear(64, 64)),\n",
        "            nn.Tanh(),\n",
        "            layer_init(nn.Linear(64, 1), std=1.0)\n",
        "        )\n",
        "\n",
        "    def get_value(self, x):\n",
        "        return self.critic(x)\n",
        "\n",
        "    def get_action_and_value(self, x, action=None):\n",
        "        logits = self.actor(x)\n",
        "        probs = Categorical(logits=logits) # soft-max distribution\n",
        "\n",
        "        if action is None:\n",
        "            action = probs.sample()\n",
        "\n",
        "        log_prob = probs.log_prob(action)\n",
        "        entropy = probs.entropy()\n",
        "        value = self.get_value(x)\n",
        "\n",
        "        return action, log_prob, entropy, value"
      ],
      "metadata": {
        "id": "XyCdbsxNfSeK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_env(env_id, seed, idx, record_video=False):\n",
        "    def thunk():\n",
        "        env = gym.make(env_id, render_mode='human')\n",
        "        env = gym.wrappers.RecordEpisodeStatistics(env)\n",
        "        if record_video and idx == 0:\n",
        "            env = gym.wrappers.RecordVideo(env, f\"videos/{env_id}\")\n",
        "\n",
        "        env.seed(seed)\n",
        "        env.action_space.seed(seed)\n",
        "        env.observation_space.seed(seed)\n",
        "        return env\n",
        "\n",
        "    return thunk"
      ],
      "metadata": {
        "id": "SzfncvWIhIAC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = argparse.Namespace()\n",
        "\n",
        "config.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "config.env_id = 'CartPole-v1'\n",
        "config.num_envs = 4\n",
        "config.num_steps = 128     # The number of steps to run in each env to policy rollout\n",
        "\n",
        "config.batch_size = config.num_envs * config.num_steps\n",
        "config.num_minibatches = 4 # the number of mini-batches\n",
        "config.minibatch_size = config.batch_size // config.num_minibatches\n",
        "\n",
        "config.total_timesteps = 25000\n",
        "config.num_updates = config.total_timesteps // config.batch_size\n",
        "config.num_epochs = 4\n",
        "\n",
        "config.learning_rate = 3e-4\n",
        "config.gamma = 0.99\n",
        "config.anneal_lr = True    # Learning rate annealing for policy and value networks\n",
        "config.gae= True         # Generalized Advantage Estimation\n",
        "config.gae_lambda = 0.95\n",
        "\n",
        "config.norm_adv = True     # Advantages normalization\n",
        "config.clip_coef = 0.2     # The surrogate clipping coefficient (policy and value function)\n",
        "config.clip_vloss = True   # Use clip_coef to clip value function\n",
        "\n",
        "config.vf_coef = 0.5       # The value function coefficient to calculate loss\n",
        "config.ent_coef = 0.01     # Encourages the policy to explore a diverse set of actions\n",
        "config.max_grad_norm = 0.5 # The maximum norm for the gradient clipping\n",
        "config.target_kl = 0.015   # The target KL divergence threshold\n",
        "\n",
        "config.track = True"
      ],
      "metadata": {
        "id": "LfCycjEgiSFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Connect to Weights and Biases"
      ],
      "metadata": {
        "id": "zMWeN-zCwPAn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if config.track:\n",
        "    !pip install wandb"
      ],
      "metadata": {
        "id": "ilcRVfs1xChY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if config.track:\n",
        "    import wandb\n",
        "    wandb.login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FqEOt34wwJ1o",
        "outputId": "6cc0e94d-d47e-45d0-e4a6-160d3420fcbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if config.track:\n",
        "    wandb.init(\n",
        "        project='ppo-implementation-details',\n",
        "        config=config,\n",
        "        name=config.env_id,\n",
        "        monitor_gym=True,\n",
        "        save_code=True,\n",
        "    )"
      ],
      "metadata": {
        "id": "z2jmW0zXkI7g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Seeding\n",
        "random.seed(config.seed)\n",
        "np.random.seed(config.seed)\n",
        "torch.manual_seed(config.seed)\n",
        "\n",
        "envs = gym.vector.SyncVectorEnv(\n",
        "    [make_env(config.env_id, config.seed + i, i, record_video=True) for i in range(config.num_envs)]\n",
        ")\n",
        "\n",
        "print(\"Observation space:\", envs.single_observation_space)\n",
        "print(\"Action space:\", envs.single_action_space)\n",
        "\n",
        "assert isinstance(envs.single_action_space, gym.spaces.Discrete), \"only discrete action space is supported\"\n",
        "\n",
        "agent = PPOAgent(envs).to(config.device)\n",
        "optimizer = optim.Adam(agent.parameters(), lr=config.learning_rate, eps=1e-5)"
      ],
      "metadata": {
        "id": "t_IW5a-UjChA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_gae_advantages(rewards, values, next_return, dones, num_steps, gamma=0.99, gae_lambda=0.95):\n",
        "    gae_advantages = torch.zeros_like(rewards)\n",
        "\n",
        "    gae = 0\n",
        "    for t in reversed(range(num_steps)):\n",
        "        delta = rewards[t] + gamma * next_return * (1 - dones[t]) - values[t]\n",
        "        gae = delta + gamma * gae_lambda * gae * (1 - dones[t])\n",
        "        gae_advantages[t] = gae\n",
        "        next_return = values[t]\n",
        "\n",
        "    returns = gae_advantages + values\n",
        "    return returns, gae_advantages"
      ],
      "metadata": {
        "id": "mzGAXAaiHVlA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_n_step_return(rewards, values, next_return, dones, num_steps, gamma=0.99):\n",
        "      returns = torch.zeros_like(rewards)\n",
        "\n",
        "      for t in reversed(range(num_steps)):\n",
        "          returns[t] = rewards[t] + gamma * next_return * (1 - dones[t])\n",
        "          next_return = returns[t]\n",
        "\n",
        "      advantages = returns - values\n",
        "      return returns, advantages"
      ],
      "metadata": {
        "id": "8Afnt2goNSyh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def PPOTrain(envs, optimizer, config):\n",
        "  global global_step\n",
        "\n",
        "  # Storage setup\n",
        "  observations = torch.zeros((config.num_steps, config.num_envs) + envs.single_observation_space.shape).to(config.device)\n",
        "  actions = torch.zeros((config.num_steps, config.num_envs) + envs.single_action_space.shape, dtype=torch.long).to(config.device)\n",
        "  rewards = torch.zeros((config.num_steps, config.num_envs)).to(config.device)\n",
        "  dones = torch.zeros((config.num_steps, config.num_envs)).to(config.device)\n",
        "  values = torch.zeros((config.num_steps, config.num_envs)).to(config.device)\n",
        "  logprobs = torch.zeros((config.num_steps, config.num_envs)).to(config.device)\n",
        "\n",
        "  for update in range(config.num_updates):\n",
        "      # Update learning rate\n",
        "      if config.anneal_lr:\n",
        "          lr_frac = 1.0 - update / config.num_updates\n",
        "          optimizer.param_groups[0][\"lr\"] = lr_frac * config.learning_rate\n",
        "\n",
        "      # Do n-steps\n",
        "      obs = envs.reset()\n",
        "      for step in range(config.num_steps):\n",
        "          global_step += config.num_envs\n",
        "\n",
        "          with torch.no_grad():\n",
        "              obs_tensor = torch.Tensor(obs).to(config.device)\n",
        "              action, logprob, _, value = agent.get_action_and_value(obs_tensor)\n",
        "\n",
        "          next_obs, reward, done, info = envs.step(action.cpu().numpy())\n",
        "\n",
        "          # Save batch\n",
        "          observations[step] = obs_tensor\n",
        "          actions[step] = torch.Tensor(action, dtype=torch.long).to(config.device)\n",
        "          rewards[step] = torch.Tensor(reward).to(config.device)\n",
        "          dones[step] = torch.Tensor(done).to(config.device)\n",
        "          values[step] = torch.Tensor(value.flatten()).to(config.device)\n",
        "          logprobs[step] = torch.Tensor(logprob).to(config.device)\n",
        "\n",
        "          if info.get('episode') is not None:\n",
        "              episodic_info = info.get('episode')[0]\n",
        "              if episodic_info is not None:\n",
        "                  print(f\"global_step={global_step}, episodic_return={episodic_info['r']}\")\n",
        "                  wandb.log({'episodic_return': episodic_info['r']})\n",
        "\n",
        "          obs = next_obs\n",
        "\n",
        "      # Generalized Advantage Estimation\n",
        "      with torch.no_grad():\n",
        "          next_obs_tensor = torch.Tensor(next_obs).to(config.device)\n",
        "          next_value = agent.get_value(next_obs_tensor).reshape(1, -1)\n",
        "\n",
        "          if config.gae:\n",
        "              returns, advantages = compute_gae_advantages(rewards, values, next_value, dones, config.num_steps, config.gamma, config.gae_lambda)\n",
        "          else:\n",
        "              returns, advantages = compute_n_step_return(rewards, values, next_value, dones, config.num_steps, config.gamma)\n",
        "\n",
        "      # Flatten the batch: num_steps * num_envs\n",
        "      b_observations = observations.reshape((-1,) + envs.single_observation_space.shape)\n",
        "      b_actions = actions.reshape((-1,) + envs.single_action_space.shape)\n",
        "      b_returns = returns.reshape(-1)\n",
        "      b_values = values.reshape(-1)\n",
        "      b_logprobs = logprobs.reshape(-1)\n",
        "      b_advantages = advantages.reshape(-1)\n",
        "\n",
        "      clip_fracs = []\n",
        "\n",
        "      # Shuffles the indices of the batch and breaks it into mini-batches\n",
        "      batch_inds = np.arange(config.batch_size)\n",
        "      for epoch in range(config.num_epochs):\n",
        "          np.random.shuffle(batch_inds)\n",
        "\n",
        "          for start in range(0, config.batch_size, config.minibatch_size):\n",
        "              end = start + config.minibatch_size\n",
        "              minibatch_inds = batch_inds[start:end]\n",
        "\n",
        "              # Mini-batches: targets\n",
        "              mb_observations = b_observations[minibatch_inds]\n",
        "              mb_actions = b_actions[minibatch_inds]\n",
        "              mb_returns = b_returns[minibatch_inds]\n",
        "              mb_values = b_values[minibatch_inds]\n",
        "              mb_logprobs = b_logprobs[minibatch_inds]\n",
        "              mb_advantages = b_advantages[minibatch_inds]\n",
        "\n",
        "              # Advantages normalization\n",
        "              if config.norm_adv:\n",
        "                  mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)\n",
        "\n",
        "              # Predictions\n",
        "              _, mb_logprobs_pred, mb_entropy_pred, mb_values_pred = agent.get_action_and_value(mb_observations, mb_actions)\n",
        "\n",
        "              log_ratio = mb_logprobs_pred - mb_logprobs\n",
        "              ratio = log_ratio.exp()\n",
        "\n",
        "              with torch.no_grad():\n",
        "                  clip_fracs += [((ratio - 1.0).abs() > config.clip_coef).float().mean().item()]\n",
        "\n",
        "              # Policy loss\n",
        "              pg_loss_unclipped = -mb_advantages * ratio\n",
        "              pg_loss_clipped = -mb_advantages * torch.clamp(ratio, 1 - config.clip_coef, 1 + config.clip_coef)\n",
        "              pg_loss = torch.max(pg_loss_unclipped, pg_loss_clipped).mean()\n",
        "\n",
        "              # Value loss\n",
        "              mb_values_pred = mb_values_pred.view(-1)\n",
        "\n",
        "              v_loss_sqrt = (mb_values_pred - mb_returns) ** 2 # Unclipped\n",
        "              if config.clip_vloss:\n",
        "                  # Ensure that the value function updates do not deviate too far from the original values\n",
        "                  v_clipped = torch.clamp(mb_values_pred, mb_values - config.clip_coef, mb_values + config.clip_coef)\n",
        "                  v_loss_clipped = (v_clipped - mb_returns) ** 2\n",
        "                  v_loss_sqrt = torch.max(v_loss_sqrt, v_loss_clipped)\n",
        "\n",
        "              v_loss = 0.5 * v_loss_sqrt.mean()\n",
        "\n",
        "              # Entropy Loss\n",
        "              entropy_loss = mb_entropy_pred.mean()\n",
        "\n",
        "              # Overall Loss\n",
        "              loss = pg_loss - config.ent_coef * entropy_loss + config.vf_coef * v_loss\n",
        "\n",
        "              optimizer.zero_grad()\n",
        "              loss.backward()\n",
        "\n",
        "              # Global Gradient Clipping\n",
        "              nn.utils.clip_grad_norm_(agent.parameters(), config.max_grad_norm)\n",
        "\n",
        "              optimizer.step()\n",
        "\n",
        "          with torch.no_grad():\n",
        "            # old_approx_kl = (-logratio).mean()\n",
        "            approx_kl = ((ratio - 1) - log_ratio).mean()\n",
        "\n",
        "          # Early stop using approx_kl\n",
        "          if config.target_kl is not None:\n",
        "                if approx_kl > config.target_kl:\n",
        "                    break\n",
        "\n",
        "      y_pred, y_true = b_values.cpu().numpy(), b_returns.cpu().numpy()\n",
        "      var_y = np.var(y_true)\n",
        "      explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y\n",
        "\n",
        "      if config.track:\n",
        "          metrics = {\n",
        "              \"GLOBAL STEP\": global_step,\n",
        "              \"loss\": loss.item(),\n",
        "              \"learning_rate\": optimizer.param_groups[0][\"lr\"],\n",
        "              \"value_loss\": v_loss.item(),\n",
        "              \"policy_loss\": pg_loss.item(),\n",
        "              \"entropy\": entropy_loss.item(),\n",
        "              \"approx_kl\": approx_kl.item(),\n",
        "              \"clip_frac\": np.mean(clip_fracs),\n",
        "              \"explained_variance\": explained_var\n",
        "          }\n",
        "          wandb.log(metrics)\n",
        "          print(metrics)"
      ],
      "metadata": {
        "id": "Nobj6RHd0hWf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "global_step = 0"
      ],
      "metadata": {
        "id": "6PTu8er9r_Vg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PPOTrain(envs, optimizer, config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CL_nAYeRoHNp",
        "outputId": "fff7e656-6d44-472e-d962-b67b25383446"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/monitoring/video_recorder.py:89: UserWarning: \u001b[33mWARN: Disabling video recorder because environment <RecordEpisodeStatistics<TimeLimit<OrderEnforcing<StepAPICompatibility<PassiveEnvChecker<CartPoleEnv<CartPole-v1>>>>>>> was not initialized with any compatible video mode between `single_rgb_array` and `rgb_array`\u001b[0m\n",
            "  logger.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "global_step=80, episodic_return=20.0\n",
            "global_step=128, episodic_return=12.0\n",
            "global_step=184, episodic_return=14.0\n",
            "global_step=300, episodic_return=29.0\n",
            "global_step=456, episodic_return=39.0\n",
            "{'GLOBAL STEP': 512, 'loss': 20.69984245300293, 'learning_rate': 0.0003, 'value_loss': 41.413299560546875, 'policy_loss': 0.00012090802192687988, 'entropy': 0.6928695440292358, 'approx_kl': 0.00027229078114032745, 'clip_frac': 0.0, 'explained_variance': -0.0026308298110961914}\n",
            "global_step=616, episodic_return=26.0\n",
            "global_step=668, episodic_return=13.0\n",
            "global_step=764, episodic_return=24.0\n",
            "global_step=900, episodic_return=34.0\n",
            "global_step=1012, episodic_return=28.0\n",
            "{'GLOBAL STEP': 1024, 'loss': 16.502805709838867, 'learning_rate': 0.00029374999999999996, 'value_loss': 33.03014373779297, 'policy_loss': -0.005356862209737301, 'entropy': 0.6910135746002197, 'approx_kl': 0.0007931031286716461, 'clip_frac': 0.0, 'explained_variance': -0.0010976791381835938}\n",
            "global_step=1188, episodic_return=41.0\n",
            "global_step=1412, episodic_return=56.0\n",
            "{'GLOBAL STEP': 1536, 'loss': 27.34903907775879, 'learning_rate': 0.0002875, 'value_loss': 54.721744537353516, 'policy_loss': -0.004942629486322403, 'entropy': 0.6891284584999084, 'approx_kl': 0.0006300122477114201, 'clip_frac': 0.0, 'explained_variance': -0.0022118091583251953}\n",
            "global_step=1624, episodic_return=22.0\n",
            "global_step=1684, episodic_return=15.0\n",
            "global_step=1728, episodic_return=11.0\n",
            "global_step=1868, episodic_return=35.0\n",
            "global_step=1920, episodic_return=13.0\n",
            "{'GLOBAL STEP': 2048, 'loss': 19.885461807250977, 'learning_rate': 0.00028125, 'value_loss': 39.81106948852539, 'policy_loss': -0.013277344405651093, 'entropy': 0.6795791983604431, 'approx_kl': 0.0015584558714181185, 'clip_frac': 0.0, 'explained_variance': 0.009084701538085938}\n",
            "global_step=2304, episodic_return=64.0\n",
            "global_step=2388, episodic_return=21.0\n",
            "global_step=2504, episodic_return=29.0\n",
            "{'GLOBAL STEP': 2560, 'loss': 31.53348731994629, 'learning_rate': 0.00027499999999999996, 'value_loss': 63.08860397338867, 'policy_loss': -0.004101676866412163, 'entropy': 0.6712606549263, 'approx_kl': 0.0012811652850359678, 'clip_frac': 0.0, 'explained_variance': 0.0005750060081481934}\n",
            "global_step=2708, episodic_return=37.0\n",
            "global_step=2764, episodic_return=14.0\n",
            "{'GLOBAL STEP': 3072, 'loss': 29.882484436035156, 'learning_rate': 0.00026875, 'value_loss': 59.78419494628906, 'policy_loss': -0.002965308725833893, 'entropy': 0.6648525595664978, 'approx_kl': 0.0008611278608441353, 'clip_frac': 0.0, 'explained_variance': 0.004127383232116699}\n",
            "global_step=3236, episodic_return=41.0\n",
            "global_step=3424, episodic_return=47.0\n",
            "global_step=3492, episodic_return=17.0\n",
            "{'GLOBAL STEP': 3584, 'loss': 22.59026336669922, 'learning_rate': 0.0002625, 'value_loss': 45.220970153808594, 'policy_loss': -0.013676004484295845, 'entropy': 0.6546369194984436, 'approx_kl': 0.0014896364882588387, 'clip_frac': 0.0, 'explained_variance': 0.02187877893447876}\n",
            "global_step=3708, episodic_return=31.0\n",
            "global_step=3780, episodic_return=18.0\n",
            "{'GLOBAL STEP': 4096, 'loss': 31.88218879699707, 'learning_rate': 0.00025624999999999997, 'value_loss': 63.78528594970703, 'policy_loss': -0.003981238231062889, 'entropy': 0.6472796201705933, 'approx_kl': 0.001297531183809042, 'clip_frac': 0.0, 'explained_variance': 0.006738126277923584}\n",
            "{'GLOBAL STEP': 4608, 'loss': 37.03860855102539, 'learning_rate': 0.00025, 'value_loss': 74.10539245605469, 'policy_loss': -0.0077427588403224945, 'entropy': 0.6344322562217712, 'approx_kl': 0.0006503136828541756, 'clip_frac': 0.0, 'explained_variance': -0.01086270809173584}\n",
            "global_step=5104, episodic_return=124.0\n",
            "{'GLOBAL STEP': 5120, 'loss': 37.689849853515625, 'learning_rate': 0.00024375, 'value_loss': 75.40031433105469, 'policy_loss': -0.003995921462774277, 'entropy': 0.6310595273971558, 'approx_kl': 0.0004013241268694401, 'clip_frac': 0.0, 'explained_variance': 0.016811728477478027}\n",
            "global_step=5264, episodic_return=36.0\n",
            "{'GLOBAL STEP': 5632, 'loss': 34.258094787597656, 'learning_rate': 0.00023749999999999997, 'value_loss': 68.5299301147461, 'policy_loss': -0.0006700679659843445, 'entropy': 0.6201562881469727, 'approx_kl': 0.00030579930171370506, 'clip_frac': 0.0, 'explained_variance': -0.009625792503356934}\n",
            "global_step=5920, episodic_return=72.0\n",
            "{'GLOBAL STEP': 6144, 'loss': 36.920326232910156, 'learning_rate': 0.00023124999999999998, 'value_loss': 73.85658264160156, 'policy_loss': -0.001653321087360382, 'entropy': 0.6312719583511353, 'approx_kl': 0.00014444743283092976, 'clip_frac': 0.0, 'explained_variance': 0.0019724369049072266}\n",
            "global_step=6400, episodic_return=64.0\n",
            "global_step=6652, episodic_return=63.0\n",
            "{'GLOBAL STEP': 6656, 'loss': 40.687747955322266, 'learning_rate': 0.000225, 'value_loss': 81.39241027832031, 'policy_loss': -0.0021378695964813232, 'entropy': 0.631871223449707, 'approx_kl': 0.00013186992146074772, 'clip_frac': 0.0, 'explained_variance': 0.020539522171020508}\n",
            "global_step=6812, episodic_return=39.0\n",
            "global_step=7012, episodic_return=50.0\n",
            "global_step=7152, episodic_return=35.0\n",
            "{'GLOBAL STEP': 7168, 'loss': 33.23655700683594, 'learning_rate': 0.00021875, 'value_loss': 66.4820556640625, 'policy_loss': 0.0016872435808181763, 'entropy': 0.6159895658493042, 'approx_kl': 0.0002711054403334856, 'clip_frac': 0.0, 'explained_variance': 0.0274009108543396}\n",
            "global_step=7284, episodic_return=29.0\n",
            "global_step=7544, episodic_return=65.0\n",
            "{'GLOBAL STEP': 7680, 'loss': 35.918827056884766, 'learning_rate': 0.00021249999999999996, 'value_loss': 71.8472900390625, 'policy_loss': 0.0013959873467683792, 'entropy': 0.621250331401825, 'approx_kl': 0.0001054694876074791, 'clip_frac': 0.0, 'explained_variance': 0.014072597026824951}\n",
            "global_step=7756, episodic_return=19.0\n",
            "global_step=7916, episodic_return=40.0\n",
            "global_step=8184, episodic_return=67.0\n",
            "{'GLOBAL STEP': 8192, 'loss': 32.35212326049805, 'learning_rate': 0.00020624999999999997, 'value_loss': 64.72283935546875, 'policy_loss': -0.0033117979764938354, 'entropy': 0.5985580086708069, 'approx_kl': 0.00022887997329235077, 'clip_frac': 0.0, 'explained_variance': 0.016824424266815186}\n",
            "global_step=8496, episodic_return=76.0\n",
            "{'GLOBAL STEP': 8704, 'loss': 39.113773345947266, 'learning_rate': 0.0002, 'value_loss': 78.23258972167969, 'policy_loss': 0.0036029554903507233, 'entropy': 0.6123666763305664, 'approx_kl': 0.0004150127060711384, 'clip_frac': 0.0, 'explained_variance': 0.034256160259246826}\n",
            "global_step=9140, episodic_return=109.0\n",
            "{'GLOBAL STEP': 9216, 'loss': 33.250484466552734, 'learning_rate': 0.00019374999999999997, 'value_loss': 66.52140045166016, 'policy_loss': -0.004355311393737793, 'entropy': 0.5861436128616333, 'approx_kl': 0.00031191972084343433, 'clip_frac': 0.0, 'explained_variance': 0.023747503757476807}\n",
            "{'GLOBAL STEP': 9728, 'loss': 44.65216064453125, 'learning_rate': 0.00018749999999999998, 'value_loss': 89.3187026977539, 'policy_loss': -0.0011832164600491524, 'entropy': 0.600736141204834, 'approx_kl': 0.00037507805973291397, 'clip_frac': 0.0, 'explained_variance': -0.0174027681350708}\n",
            "{'GLOBAL STEP': 10240, 'loss': 39.64107131958008, 'learning_rate': 0.00018125000000000001, 'value_loss': 79.29717254638672, 'policy_loss': -0.0013842619955539703, 'entropy': 0.6131129264831543, 'approx_kl': 0.0001408006064593792, 'clip_frac': 0.0, 'explained_variance': 0.07183212041854858}\n",
            "{'GLOBAL STEP': 10752, 'loss': 43.09297180175781, 'learning_rate': 0.00017499999999999997, 'value_loss': 86.20112609863281, 'policy_loss': -0.0016263145953416824, 'entropy': 0.5965441465377808, 'approx_kl': 0.00011096918024122715, 'clip_frac': 0.0, 'explained_variance': -0.003722071647644043}\n",
            "global_step=10936, episodic_return=46.0\n",
            "{'GLOBAL STEP': 11264, 'loss': 37.562164306640625, 'learning_rate': 0.00016874999999999998, 'value_loss': 75.13843536376953, 'policy_loss': -0.0011466089636087418, 'entropy': 0.5908335447311401, 'approx_kl': 6.623798981308937e-05, 'clip_frac': 0.0, 'explained_variance': 0.03419506549835205}\n",
            "global_step=11712, episodic_return=112.0\n",
            "{'GLOBAL STEP': 11776, 'loss': 34.9669075012207, 'learning_rate': 0.00016250000000000002, 'value_loss': 69.94808959960938, 'policy_loss': -0.0012837499380111694, 'entropy': 0.5853257179260254, 'approx_kl': 6.953231059014797e-05, 'clip_frac': 0.0, 'explained_variance': 0.021222412586212158}\n",
            "global_step=11928, episodic_return=38.0\n",
            "{'GLOBAL STEP': 12288, 'loss': 44.11491012573242, 'learning_rate': 0.00015624999999999998, 'value_loss': 88.24339294433594, 'policy_loss': -0.0008067749440670013, 'entropy': 0.5978797078132629, 'approx_kl': 0.00019171717576682568, 'clip_frac': 0.0, 'explained_variance': 0.07139354944229126}\n",
            "{'GLOBAL STEP': 12800, 'loss': 43.622169494628906, 'learning_rate': 0.00015, 'value_loss': 87.25428009033203, 'policy_loss': 0.0011540791019797325, 'entropy': 0.6125531792640686, 'approx_kl': 3.9147911593317986e-05, 'clip_frac': 0.0, 'explained_variance': 0.09483540058135986}\n",
            "{'GLOBAL STEP': 13312, 'loss': 41.163673400878906, 'learning_rate': 0.00014374999999999997, 'value_loss': 82.34012603759766, 'policy_loss': -0.00047422293573617935, 'entropy': 0.5913493633270264, 'approx_kl': 7.900875061750412e-06, 'clip_frac': 0.0, 'explained_variance': 0.04927414655685425}\n",
            "{'GLOBAL STEP': 13824, 'loss': 37.219547271728516, 'learning_rate': 0.0001375, 'value_loss': 74.45175170898438, 'policy_loss': -0.0006125196814537048, 'entropy': 0.5715384483337402, 'approx_kl': 3.8086436688899994e-06, 'clip_frac': 0.0, 'explained_variance': 0.0686107873916626}\n",
            "global_step=13912, episodic_return=22.0\n",
            "{'GLOBAL STEP': 14336, 'loss': 42.928619384765625, 'learning_rate': 0.00013125, 'value_loss': 85.86809539794922, 'policy_loss': 0.0004966268315911293, 'entropy': 0.5924580693244934, 'approx_kl': 1.7002224922180176e-05, 'clip_frac': 0.0, 'explained_variance': 0.09241676330566406}\n",
            "global_step=14696, episodic_return=90.0\n",
            "{'GLOBAL STEP': 14848, 'loss': 40.692588806152344, 'learning_rate': 0.00012499999999999998, 'value_loss': 81.39473724365234, 'policy_loss': 0.0013287626206874847, 'entropy': 0.6106909513473511, 'approx_kl': 1.794169656932354e-05, 'clip_frac': 0.0, 'explained_variance': 0.10520464181900024}\n",
            "{'GLOBAL STEP': 15360, 'loss': 39.86129379272461, 'learning_rate': 0.00011875, 'value_loss': 79.72994995117188, 'policy_loss': 0.0024325549602508545, 'entropy': 0.6114708185195923, 'approx_kl': 0.00014367816038429737, 'clip_frac': 0.0, 'explained_variance': 0.03154629468917847}\n",
            "{'GLOBAL STEP': 15872, 'loss': 44.02654266357422, 'learning_rate': 0.0001125, 'value_loss': 88.06584167480469, 'policy_loss': -0.00032539665699005127, 'entropy': 0.6051939725875854, 'approx_kl': 3.875419497489929e-05, 'clip_frac': 0.0, 'explained_variance': 0.09556156396865845}\n",
            "{'GLOBAL STEP': 16384, 'loss': 37.26277542114258, 'learning_rate': 0.00010624999999999998, 'value_loss': 74.53752136230469, 'policy_loss': 2.9481947422027588e-05, 'entropy': 0.6013244986534119, 'approx_kl': 1.193024218082428e-06, 'clip_frac': 0.0, 'explained_variance': 0.07663106918334961}\n",
            "global_step=16816, episodic_return=108.0\n",
            "{'GLOBAL STEP': 16896, 'loss': 36.574161529541016, 'learning_rate': 0.0001, 'value_loss': 73.16232299804688, 'policy_loss': -0.0010331477969884872, 'entropy': 0.5966899394989014, 'approx_kl': 1.8544960767030716e-05, 'clip_frac': 0.0, 'explained_variance': 0.08342045545578003}\n",
            "global_step=17148, episodic_return=63.0\n",
            "{'GLOBAL STEP': 17408, 'loss': 38.621116638183594, 'learning_rate': 9.374999999999999e-05, 'value_loss': 77.25414276123047, 'policy_loss': -9.335577487945557e-06, 'entropy': 0.5943875312805176, 'approx_kl': 9.057112038135529e-07, 'clip_frac': 0.0, 'explained_variance': 0.09970569610595703}\n",
            "{'GLOBAL STEP': 17920, 'loss': 41.162662506103516, 'learning_rate': 8.749999999999999e-05, 'value_loss': 82.33772277832031, 'policy_loss': -0.00021414458751678467, 'entropy': 0.5984544157981873, 'approx_kl': 1.1450378224253654e-05, 'clip_frac': 0.0, 'explained_variance': 0.11700743436813354}\n",
            "global_step=18272, episodic_return=88.0\n",
            "{'GLOBAL STEP': 18432, 'loss': 37.027015686035156, 'learning_rate': 8.125000000000001e-05, 'value_loss': 74.06605529785156, 'policy_loss': -0.00010299962013959885, 'entropy': 0.5907430648803711, 'approx_kl': 3.511086106300354e-07, 'clip_frac': 0.0, 'explained_variance': 0.0289609432220459}\n",
            "global_step=18640, episodic_return=52.0\n",
            "{'GLOBAL STEP': 18944, 'loss': 38.76997756958008, 'learning_rate': 7.5e-05, 'value_loss': 77.55140686035156, 'policy_loss': -1.2598931789398193e-05, 'entropy': 0.5712590217590332, 'approx_kl': 1.371372491121292e-07, 'clip_frac': 0.0, 'explained_variance': 0.04640430212020874}\n",
            "{'GLOBAL STEP': 19456, 'loss': 43.15449523925781, 'learning_rate': 6.874999999999998e-05, 'value_loss': 86.32148742675781, 'policy_loss': -0.0001192782074213028, 'entropy': 0.6129950284957886, 'approx_kl': 1.487787812948227e-07, 'clip_frac': 0.0, 'explained_variance': 0.13059473037719727}\n",
            "{'GLOBAL STEP': 19968, 'loss': 33.05669021606445, 'learning_rate': 6.25e-05, 'value_loss': 66.12525177001953, 'policy_loss': -6.481446325778961e-05, 'entropy': 0.5869758725166321, 'approx_kl': 8.544884622097015e-08, 'clip_frac': 0.0, 'explained_variance': -0.0033216476440429688}\n",
            "global_step=20096, episodic_return=32.0\n",
            "{'GLOBAL STEP': 20480, 'loss': 31.072568893432617, 'learning_rate': 5.625e-05, 'value_loss': 62.157047271728516, 'policy_loss': -0.00016406923532485962, 'entropy': 0.5790235996246338, 'approx_kl': 2.735760062932968e-07, 'clip_frac': 0.0, 'explained_variance': 0.011630475521087646}\n",
            "global_step=20600, episodic_return=30.0\n",
            "global_step=20844, episodic_return=61.0\n",
            "{'GLOBAL STEP': 20992, 'loss': 36.21024703979492, 'learning_rate': 4.999999999999998e-05, 'value_loss': 72.4322280883789, 'policy_loss': -0.0001471433788537979, 'entropy': 0.5719821453094482, 'approx_kl': 2.9313378036022186e-07, 'clip_frac': 0.0, 'explained_variance': 0.03300827741622925}\n",
            "global_step=21428, episodic_return=109.0\n",
            "{'GLOBAL STEP': 21504, 'loss': 33.544525146484375, 'learning_rate': 4.3750000000000006e-05, 'value_loss': 67.10051727294922, 'policy_loss': 8.053705096244812e-05, 'entropy': 0.5814192891120911, 'approx_kl': 2.76835635304451e-07, 'clip_frac': 0.0, 'explained_variance': 0.09129595756530762}\n",
            "global_step=21736, episodic_return=58.0\n",
            "{'GLOBAL STEP': 22016, 'loss': 36.079750061035156, 'learning_rate': 3.75e-05, 'value_loss': 72.17170715332031, 'policy_loss': -0.00019872142001986504, 'entropy': 0.590410590171814, 'approx_kl': 3.864988684654236e-07, 'clip_frac': 0.0, 'explained_variance': 0.005643784999847412}\n",
            "{'GLOBAL STEP': 22528, 'loss': 40.96284866333008, 'learning_rate': 3.124999999999999e-05, 'value_loss': 81.93760681152344, 'policy_loss': -1.2833625078201294e-05, 'entropy': 0.5940089821815491, 'approx_kl': 3.864988684654236e-08, 'clip_frac': 0.0, 'explained_variance': 0.06190687417984009}\n",
            "global_step=22992, episodic_return=116.0\n",
            "{'GLOBAL STEP': 23040, 'loss': 39.18330764770508, 'learning_rate': 2.5000000000000008e-05, 'value_loss': 78.38020324707031, 'policy_loss': -0.0008778311312198639, 'entropy': 0.5916414260864258, 'approx_kl': 2.2142194211483e-06, 'clip_frac': 0.0, 'explained_variance': 0.13807320594787598}\n",
            "{'GLOBAL STEP': 23552, 'loss': 46.74671173095703, 'learning_rate': 1.875e-05, 'value_loss': 93.50507354736328, 'policy_loss': 9.273737668991089e-05, 'entropy': 0.591765820980072, 'approx_kl': 1.3224780559539795e-07, 'clip_frac': 0.0, 'explained_variance': 0.17946022748947144}\n",
            "{'GLOBAL STEP': 24064, 'loss': 39.779964447021484, 'learning_rate': 1.2499999999999987e-05, 'value_loss': 79.57183837890625, 'policy_loss': -4.4226646423339844e-05, 'entropy': 0.5910367965698242, 'approx_kl': 3.655441105365753e-08, 'clip_frac': 0.0, 'explained_variance': 0.060268282890319824}\n",
            "global_step=24164, episodic_return=25.0\n",
            "{'GLOBAL STEP': 24576, 'loss': 41.36267852783203, 'learning_rate': 6.2500000000000105e-06, 'value_loss': 82.73689270019531, 'policy_loss': 3.976747393608093e-06, 'entropy': 0.5770227909088135, 'approx_kl': 7.916241884231567e-09, 'clip_frac': 0.0, 'explained_variance': 0.0034442543983459473}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "envs.close()\n",
        "if config.track: wandb.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        },
        "id": "NSDXZEailmi2",
        "outputId": "fd60b25e-cb81-4471-ed78-2f442e799fc6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>GLOBAL STEP</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>approx_kl</td><td>▂▅▄█▇█▇▄▃▂▂▂▁▂▃▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>clip_frac</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>entropy</td><td>███▇▇▆▅▅▄▄▄▄▄▃▃▃▃▂▂▂▃▂▁▂▃▃▃▂▂▃▁▃▂▁▁▂▂▂▂▁</td></tr><tr><td>explained_variance</td><td>▂▂▂▂▂▂▂▁▂▁▂▃▂▂▃▁▄▁▃▂▅▃▄▅▅▅▄▅▅▆▃▆▂▂▃▂▄▇█▂</td></tr><tr><td>learning_rate</td><td>████▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁</td></tr><tr><td>loss</td><td>▂▁▄▂▄▂▅▆▆▅▇▅▅▅▆█▆▇▆▅▇▇▆▇▇▇▆▆▆▇▆▇▅▄▆▆▇▆█▇</td></tr><tr><td>policy_loss</td><td>▇▄▅▁▅▁▅▃▅▆▆▇▇▅█▆▆▆▆▆▇▆▆▇▇▆▇▆▇▆▇▆▇▆▆▆▇▆▇▇</td></tr><tr><td>value_loss</td><td>▂▁▄▂▄▂▅▆▆▅▇▅▅▅▆█▆▇▆▅▇▇▆▇▇▇▆▆▆▇▆▇▅▄▆▆▇▆█▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>GLOBAL STEP</td><td>24576</td></tr><tr><td>approx_kl</td><td>0.0</td></tr><tr><td>clip_frac</td><td>0.0</td></tr><tr><td>entropy</td><td>0.57702</td></tr><tr><td>explained_variance</td><td>0.00344</td></tr><tr><td>learning_rate</td><td>1e-05</td></tr><tr><td>loss</td><td>41.36268</td></tr><tr><td>policy_loss</td><td>0.0</td></tr><tr><td>value_loss</td><td>82.73689</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">CartPole-v1</strong> at: <a href='https://wandb.ai/jupit/ppo-implementation-details/runs/0tnt4bdl' target=\"_blank\">https://wandb.ai/jupit/ppo-implementation-details/runs/0tnt4bdl</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20230801_202234-0tnt4bdl/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}