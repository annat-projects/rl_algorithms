{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Implementation details MultiDiscrete action space\n",
        "\n",
        "The multi-discrete action space consists of a series of discrete action spaces with different number of actions in each.\n",
        "\n",
        "In MultiDiscrete action spaces, the actions are independent action components, that means the agent can take multiple discrete actions simultaneously.\n",
        "\n",
        "To account for this difference, PPO treats [a1,a2] as probabilistically independent action components, therefore calculating prob(a)=prob(a1)â‹…prob(a2)\n"
      ],
      "metadata": {
        "id": "a2npAuKuNdVM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gym\n",
        "!pip install gym_microrts==0.3.2\n",
        "!pip install pyvirtualdisplay\n",
        "!pip install -y xvfb ffmpeg"
      ],
      "metadata": {
        "id": "EXVe7mUmdEgi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import random\n",
        "import gym\n",
        "import gym_microrts\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.distributions.categorical import Categorical"
      ],
      "metadata": {
        "id": "41HxvtcJgmvb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "380bf4e4-d7b9-4f66-ad39-da924f79d6d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py:440: UserWarning: \u001b[33mWARN: The `registry.env_specs` property along with `EnvSpecTree` is deprecated. Please use `registry` directly as a dictionary instead.\u001b[0m\n",
            "  logger.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
        "    nn.init.orthogonal_(layer.weight, std)\n",
        "    nn.init.constant_(layer.bias, bias_const)\n",
        "    return layer\n",
        "\n",
        "def conv2d_output_size(W_in, kernel_size, stride=1):\n",
        "    \"\"\"Compute the output size of a 2D convolution operation.\"\"\"\n",
        "    return (W_in - (kernel_size - 1) - 1) // stride + 1"
      ],
      "metadata": {
        "id": "WHt_EgEWdxax",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ada93aaf-8c78-40a0-92ac-6a381d54171b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Transpose(nn.Module):\n",
        "    def __init__(self, permutation):\n",
        "        super().__init__()\n",
        "        self.permutation = permutation\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x.permute(self.permutation)"
      ],
      "metadata": {
        "id": "FkFRQtQ-VJWS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PPOAgent(nn.Module):\n",
        "    def __init__(self, state_shape, action_nvec):\n",
        "        super(PPOAgent, self).__init__()\n",
        "\n",
        "        self.action_nvec = action_nvec\n",
        "\n",
        "        conv1_size = conv2d_output_size(state_shape[0], kernel_size=3, stride=2)\n",
        "        conv2_size = conv2d_output_size(conv1_size, kernel_size=2, stride=1)\n",
        "\n",
        "        self.network = nn.Sequential(\n",
        "            Transpose((0, 3, 1, 2)), # transpose: batch_size, channels in, height, weight. 8 x 27x16x16\n",
        "            layer_init(nn.Conv2d(state_shape[2], 16, kernel_size=3, stride=2)), # 8 x 16x7x7\n",
        "            nn.ReLU(),\n",
        "            layer_init(nn.Conv2d(16, 32, kernel_size=2, stride=1)), # 8 x 32x6x6\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(), # 8 x 1152\n",
        "            layer_init(nn.Linear(32*conv2_size*conv2_size, 128)), # 8 x 128\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.actor = layer_init(nn.Linear(128, action_nvec.sum()), std=0.01)\n",
        "        self.critic = layer_init(nn.Linear(128, 1), std=1.0)\n",
        "\n",
        "    def get_value(self, x):\n",
        "        hidden = self.network(x)\n",
        "        return self.critic(hidden)\n",
        "\n",
        "    def get_action_and_value(self, x, action=None):\n",
        "        hidden = self.network(x)\n",
        "        logits = self.actor(hidden)\n",
        "        split_logits = torch.split(logits, self.action_nvec.tolist(), dim=1)\n",
        "        multi_probs = [Categorical(logits=logits) for logits in split_logits]\n",
        "\n",
        "        if action is None:\n",
        "            action = torch.stack([probs.sample() for probs in multi_probs])\n",
        "\n",
        "        log_prob = torch.stack([probs.log_prob(a) for a, probs in zip(action, multi_probs)]).sum(0)\n",
        "        entropy = torch.stack([probs.entropy() for probs in multi_probs]).sum(0)\n",
        "        value = self.critic(hidden)\n",
        "\n",
        "        return action.T, log_prob, entropy, value"
      ],
      "metadata": {
        "id": "XyCdbsxNfSeK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = argparse.Namespace()\n",
        "\n",
        "config.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "config.seed = 1\n",
        "\n",
        "config.env_id = 'MicrortsMining-v4'\n",
        "config.num_envs = 8\n",
        "config.num_steps = 128     # The number of steps to run in each env to policy rollout\n",
        "\n",
        "config.batch_size = config.num_envs * config.num_steps\n",
        "config.num_minibatches = 4 # the number of mini-batches\n",
        "config.minibatch_size = config.batch_size // config.num_minibatches\n",
        "\n",
        "config.total_timesteps = 2000000\n",
        "config.num_updates = config.total_timesteps // config.batch_size\n",
        "config.num_epochs = 4\n",
        "\n",
        "config.learning_rate = 3e-4\n",
        "config.gamma = 0.99\n",
        "config.anneal_lr = True    # Learning rate annealing for policy and value networks\n",
        "config.gae = True         # Generalized Advantage Estimation\n",
        "config.gae_lambda = 0.95\n",
        "\n",
        "config.norm_adv = True     # Advantages normalization\n",
        "config.clip_coef = 0.1     # The surrogate clipping coefficient (policy and value function)\n",
        "config.clip_vloss = True   # Use clip_coef to clip value function\n",
        "\n",
        "config.vf_coef = 0.5       # The value coefficient to calculate loss\n",
        "config.ent_coef = 0.01     # Encourages the policy to explore a diverse set of actions\n",
        "config.max_grad_norm = 0.5 # The maximum normalization for the gradient clipping\n",
        "config.target_kl = 0.015   # The target KL divergence threshold\n",
        "\n",
        "config.track = True\n",
        "config.record_video = False"
      ],
      "metadata": {
        "id": "LfCycjEgiSFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Connect to Weights and Biases"
      ],
      "metadata": {
        "id": "zMWeN-zCwPAn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if config.track:\n",
        "    !pip install wandb"
      ],
      "metadata": {
        "id": "FcGQTrTCdcFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if config.track:\n",
        "    import wandb\n",
        "    wandb.login()"
      ],
      "metadata": {
        "id": "FqEOt34wwJ1o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if config.track:\n",
        "    wandb.init(\n",
        "        project='ppo-implementation-details',\n",
        "        config=config,\n",
        "        name=config.env_id,\n",
        "        monitor_gym=True,\n",
        "        save_code=True,\n",
        "    )"
      ],
      "metadata": {
        "id": "z2jmW0zXkI7g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_env(env_id, seed, idx, record_video=False):\n",
        "    def thunk():\n",
        "        env = gym.make(env_id)\n",
        "        if record_video and idx == 0:\n",
        "            env = gym.wrappers.RecordVideo(env, f\"videos/{env_id}\")\n",
        "\n",
        "        env.seed(seed)\n",
        "        env.action_space.seed(seed)\n",
        "        env.observation_space.seed(seed)\n",
        "\n",
        "        return env\n",
        "\n",
        "    return thunk"
      ],
      "metadata": {
        "id": "BFN64h8IiZri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Seeding\n",
        "random.seed(config.seed)\n",
        "np.random.seed(config.seed)\n",
        "torch.manual_seed(config.seed)\n",
        "\n",
        "envs = gym.vector.SyncVectorEnv(\n",
        "    [make_env(config.env_id, config.seed + i, i, record_video=config.record_video) for i in range(config.num_envs)]\n",
        ")\n",
        "\n",
        "assert isinstance(envs.single_action_space, gym.spaces.MultiDiscrete), \"only milti discrete action space is supported\"\n",
        "\n",
        "print(\"Observation space:\", envs.single_observation_space)\n",
        "print(\"Action space:\", envs.single_action_space.nvec)\n",
        "\n",
        "agent = PPOAgent(envs.single_observation_space.shape, envs.single_action_space.nvec).to(config.device)\n",
        "optimizer = optim.Adam(agent.parameters(), lr=config.learning_rate, eps=1e-5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_IW5a-UjChA",
        "outputId": "00aa8feb-b66b-47f8-d02a-6f7513c716e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Observation space: Box(0, 1, (16, 16, 27), int32)\n",
            "Action space: [256   6   4   4   4   4   7 256]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_gae_advantages(rewards, values, next_return, dones, num_steps, gamma=0.99, gae_lambda=0.95):\n",
        "    gae_advantages = torch.zeros_like(rewards)\n",
        "\n",
        "    gae = 0\n",
        "    for t in reversed(range(num_steps)):\n",
        "        delta = rewards[t] + gamma * next_return * (1 - dones[t]) - values[t]\n",
        "        gae = delta + gamma * gae_lambda * gae * (1 - dones[t])\n",
        "        gae_advantages[t] = gae\n",
        "        next_return = values[t]\n",
        "\n",
        "    returns = gae_advantages + values\n",
        "    return returns, gae_advantages"
      ],
      "metadata": {
        "id": "mzGAXAaiHVlA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_n_step_return(rewards, values, next_return, dones, num_steps, gamma=0.99):\n",
        "    returns = torch.zeros_like(rewards)\n",
        "\n",
        "    for t in reversed(range(num_steps)):\n",
        "        returns[t] = rewards[t] + gamma * next_return * (1 - dones[t])\n",
        "        next_return = returns[t]\n",
        "\n",
        "    advantages = returns - values\n",
        "    return returns, advantages"
      ],
      "metadata": {
        "id": "8Afnt2goNSyh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def PPOTrain(envs, optimizer, config):\n",
        "  global global_step\n",
        "\n",
        "  # Storage setup\n",
        "  observations = torch.zeros((config.num_steps, config.num_envs) + envs.single_observation_space.shape).to(config.device)\n",
        "  actions = torch.zeros((config.num_steps, config.num_envs) + envs.single_action_space.shape, dtype=torch.long).to(config.device)\n",
        "  rewards = torch.zeros((config.num_steps, config.num_envs)).to(config.device)\n",
        "  dones = torch.zeros((config.num_steps, config.num_envs)).to(config.device)\n",
        "  values = torch.zeros((config.num_steps, config.num_envs)).to(config.device)\n",
        "  logprobs = torch.zeros((config.num_steps, config.num_envs)).to(config.device)\n",
        "\n",
        "  obs = envs.reset()\n",
        "\n",
        "  for update in range(config.num_updates):\n",
        "      # Update learning rate\n",
        "      if config.anneal_lr:\n",
        "          lr_frac = 1.0 - update / config.num_updates\n",
        "          optimizer.param_groups[0][\"lr\"] = lr_frac * config.learning_rate\n",
        "\n",
        "      # Do n-steps\n",
        "      episodic_return = 0\n",
        "      for step in range(config.num_steps):\n",
        "          global_step += config.num_envs\n",
        "\n",
        "          with torch.no_grad():\n",
        "              obs_tensor = torch.Tensor(obs).to(config.device)\n",
        "              action, logprob, _, value = agent.get_action_and_value(obs_tensor)\n",
        "\n",
        "          next_obs, reward, done, _, = envs.step(action.cpu().numpy())\n",
        "\n",
        "          # Save batch\n",
        "          observations[step] = obs_tensor\n",
        "          actions[step] = torch.tensor(action, dtype=torch.long).to(config.device)\n",
        "          rewards[step] = torch.Tensor(reward).to(config.device)\n",
        "          dones[step] = torch.Tensor(done).to(config.device)\n",
        "          values[step] = torch.Tensor(value.flatten()).to(config.device)\n",
        "          logprobs[step] = torch.Tensor(logprob).to(config.device)\n",
        "\n",
        "          episodic_return += reward.mean()\n",
        "          obs = next_obs\n",
        "\n",
        "      print(f\"global_step={global_step}, episodic_return={episodic_return}\")\n",
        "      if config.track:\n",
        "          wandb.log({'episodic_return': episodic_return})\n",
        "\n",
        "      # Generalized Advantage Estimation\n",
        "      with torch.no_grad():\n",
        "          next_obs_tensor = torch.Tensor(next_obs).to(config.device)\n",
        "          next_value = agent.get_value(next_obs_tensor).reshape(1, -1)\n",
        "\n",
        "          if config.gae:\n",
        "              returns, advantages = compute_gae_advantages(rewards, values, next_value, dones, config.num_steps, config.gamma, config.gae_lambda)\n",
        "          else:\n",
        "              returns, advantages = compute_n_step_return(rewards, values, next_value, dones, config.num_steps, config.gamma)\n",
        "\n",
        "      # Flatten the batch: num_steps * num_envs\n",
        "      b_observations = observations.reshape((-1,) + envs.single_observation_space.shape)\n",
        "      b_actions = actions.reshape((-1,) + envs.single_action_space.shape)\n",
        "      b_returns = returns.reshape(-1)\n",
        "      b_values = values.reshape(-1)\n",
        "      b_logprobs = logprobs.reshape(-1)\n",
        "      b_advantages = advantages.reshape(-1)\n",
        "\n",
        "      clip_fracs = []\n",
        "\n",
        "      # Shuffles the indices of the batch and breaks it into mini-batches\n",
        "      batch_inds = np.arange(config.batch_size)\n",
        "      for epoch in range(config.num_epochs):\n",
        "          np.random.shuffle(batch_inds)\n",
        "\n",
        "          for start in range(0, config.batch_size, config.minibatch_size):\n",
        "              end = start + config.minibatch_size\n",
        "              minibatch_inds = batch_inds[start:end]\n",
        "\n",
        "              # Mini-batches: targets\n",
        "              mb_observations = b_observations[minibatch_inds]\n",
        "              mb_actions = b_actions[minibatch_inds]\n",
        "              mb_returns = b_returns[minibatch_inds]\n",
        "              mb_values = b_values[minibatch_inds]\n",
        "              mb_logprobs = b_logprobs[minibatch_inds]\n",
        "              mb_advantages = b_advantages[minibatch_inds]\n",
        "\n",
        "              # Advantages normalization\n",
        "              if config.norm_adv:\n",
        "                  mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)\n",
        "\n",
        "              # Predictions\n",
        "              _, mb_logprobs_pred, mb_entropy_pred, mb_values_pred = agent.get_action_and_value(mb_observations, mb_actions.T)\n",
        "\n",
        "              log_ratio = mb_logprobs_pred - mb_logprobs\n",
        "              ratio = log_ratio.exp()\n",
        "\n",
        "              with torch.no_grad():\n",
        "                  clip_fracs += [((ratio - 1.0).abs() > config.clip_coef).float().mean().item()]\n",
        "\n",
        "              # Policy loss\n",
        "              pg_loss_unclipped = -mb_advantages * ratio\n",
        "              pg_loss_clipped = -mb_advantages * torch.clamp(ratio, 1 - config.clip_coef, 1 + config.clip_coef)\n",
        "              pg_loss = torch.max(pg_loss_unclipped, pg_loss_clipped).mean()\n",
        "\n",
        "              # Value loss\n",
        "              mb_values_pred = mb_values_pred.view(-1)\n",
        "\n",
        "              v_loss_sqrt = (mb_values_pred - mb_returns) ** 2 # Unclipped\n",
        "              if config.clip_vloss:\n",
        "                  # Ensure that the value function updates do not deviate too far from the original values\n",
        "                  v_clipped = torch.clamp(mb_values_pred, mb_values - config.clip_coef, mb_values + config.clip_coef)\n",
        "                  v_loss_clipped = (v_clipped - mb_returns) ** 2\n",
        "                  v_loss_sqrt = torch.max(v_loss_sqrt, v_loss_clipped)\n",
        "\n",
        "              v_loss = 0.5 * v_loss_sqrt.mean()\n",
        "\n",
        "              # Entropy Loss\n",
        "              entropy_loss = mb_entropy_pred.mean()\n",
        "\n",
        "              # Overall Loss\n",
        "              loss = pg_loss - config.ent_coef * entropy_loss + config.vf_coef * v_loss\n",
        "\n",
        "              optimizer.zero_grad()\n",
        "              loss.backward()\n",
        "\n",
        "              # Global Gradient Clipping\n",
        "              nn.utils.clip_grad_norm_(agent.parameters(), config.max_grad_norm)\n",
        "\n",
        "              optimizer.step()\n",
        "\n",
        "          with torch.no_grad():\n",
        "            # old_approx_kl = (-logratio).mean()\n",
        "            approx_kl = ((ratio - 1) - log_ratio).mean()\n",
        "\n",
        "          # Early stop using approx_kl\n",
        "          if config.target_kl is not None:\n",
        "                if approx_kl > config.target_kl:\n",
        "                    break\n",
        "\n",
        "      y_pred, y_true = b_values.cpu().numpy(), b_returns.cpu().numpy()\n",
        "      var_y = np.var(y_true)\n",
        "      explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y\n",
        "\n",
        "      if config.track:\n",
        "          metrics = {\n",
        "              \"GLOBAL STEP\": global_step,\n",
        "              \"loss\": loss.item(),\n",
        "              \"learning_rate\": optimizer.param_groups[0][\"lr\"],\n",
        "              \"value_loss\": v_loss.item(),\n",
        "              \"policy_loss\": pg_loss.item(),\n",
        "              \"entropy\": entropy_loss.item(),\n",
        "              \"approx_kl\": approx_kl.item(),\n",
        "              \"clip_frac\": np.mean(clip_fracs),\n",
        "              \"explained_variance\": explained_var\n",
        "          }\n",
        "          wandb.log(metrics)\n",
        "          print(metrics)"
      ],
      "metadata": {
        "id": "Nobj6RHd0hWf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "global_step = 0"
      ],
      "metadata": {
        "id": "6PTu8er9r_Vg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PPOTrain(envs, optimizer, config)"
      ],
      "metadata": {
        "id": "CL_nAYeRoHNp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(num_steps):\n",
        "    episodic_return = 0\n",
        "    state = envs.reset()\n",
        "\n",
        "    for step in range(num_steps):\n",
        "        with torch.no_grad():\n",
        "            state = torch.Tensor(state).to(config.device)\n",
        "            action, _, _, _ = agent.get_action_and_value(state)\n",
        "\n",
        "        state, reward, done, _, = envs.step(action.cpu().numpy())\n",
        "\n",
        "        episodic_return += reward.mean()\n",
        "        video_recorder.capture_frame()\n",
        "\n",
        "        if done: break\n",
        "\n",
        "    video_recorder.close()"
      ],
      "metadata": {
        "id": "6fFupna3ibzv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyvirtualdisplay import Display\n",
        "from gym.wrappers.monitoring.video_recorder import VideoRecorder\n",
        "\n",
        "display = Display(visible=False, size=(1400, 900))\n",
        "_ = display.start()\n",
        "\n",
        "envs.render_mode = 'rgb_array'\n",
        "video_path = f\"./videos/{config.env_id}.mp4\"\n",
        "video_recorder = VideoRecorder(envs, video_path, enabled=True)\n",
        "\n",
        "evaluate(2000)"
      ],
      "metadata": {
        "id": "qxwRbjw7j52G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, HTML\n",
        "# Display the video\n",
        "display(HTML(f\"\"\"<video src=\"{video_path}\" width=400 controls></video>\"\"\"))"
      ],
      "metadata": {
        "id": "_7aaMG3aOpeE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "envs.close()\n",
        "if config.track: wandb.finish()"
      ],
      "metadata": {
        "id": "apQ1X8Y7kamn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}